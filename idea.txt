Want to try out(high->low):
-smart parameter tuning on anytime algorithms(mby use statistical tests to help with time)

-General intelligence as maximizing future options? interesting concept(maybe use as way to tag samples-increase future options good and decrease bad on 1:2 ratio)
->maybe add deep learning(learn deep concept/good representation from simple representation(i.e. pixels)->good with sensors. can also work with unlabled data)
->what the heck does "future options" even mean in most contexts we use? Is this generalizable? relation to simple biological functions?
->use PyBrain here?
-reinforcment learning(fit agent to enviroment+task, get feedback from enviroment, repeat)
-cognitive systems, IBM watson

-NLP understanding: creating and linking concepts using texts(differing confidence?) and core concepts(biology?)
-How to create good concepts and good linking relations?+how to defer concept until better understand or refine?
-Can mby combine with ESA(download from Evgeny webpage link to implementations) for deep understanding(could look at word, word+2 nearest, then 4 nearest and so on until sentence. this is like POS tagging)
-information extraction? open IE? (isn't this basically it in concept?)(parse(can get away with rougher parsing)+coreference of words)
-freebase knowledge graph+open linked data
-deep learning with NLP using encoding schemes? and mby can expand for active case+have learning rate depend on number of examples...
-cognitive systems, IBM watson

-deeep learning(Neural nets, belief nets, boltzman) as feature generation, mby try deep learning with relational data mixed in somehow?

-extending my work for handeling general set-valued features? seems like a mostly natural expansion...

Minor stuff:
-Anytime+Online/active learning (just like the brain)->one example/minibatch at a time and interuptable time to learn. should maybe adapt only after a few examples were wrong?
-methods of collective classification(semi-supervised learning)-especially when both partial links and partial tagging. RDF data has graph-based kernels (markov logic? markov/bayes networks mby enough?)
-Deep learning with other weight schemes(not convolution/full connect)(dropout zeros some fraction of the weights and helps generalize so mby)? also other transforms?
-One examplar Deep learning neural nets that do well? mby after already trained on some known classes and got a new one(semi-online)? or maybe train on unlabled data then get single labeled example and extrapolate well from it.
-Asymetry in mapping: learn one representation and classify with another one such that the inner product is the similarity? see nati paper
-probabalistic models better than deterministic ones since can offer confidence->can rank positive results by confidence(like in watson), especially in relational case? what do I do with this info in my life?
-self customizing google search(done by enriching query on client side? must be client side because big data too big)->see keyboards that learn style/correct such as swiftkey,swype(dragon engine),fleksy
-can learning in batches of labeled be better than giving everything all at once?-this is almost certainly theoretically impossible...
-augumented reality stuff...