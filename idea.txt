- learning visualisation stuff: embed feature space in 3-D using Johnson-Lindenstrauss? tsne v JL v SVD dimension reduction v manifold (see distilpub)

-markov networks are the most amusing shit there is

reinforcment learning:
-General intelligence as maximizing future options? interesting concept(maybe use as way to tag samples-increase future options good and decrease bad on 1:2 ratio)
->maybe add deep learning(learn deep concept/good representation from simple representation(i.e. pixels)->good with sensors. can also work with unlabled data)
->what the heck does "future options" even mean in most contexts we use? Is this generalizable? relation to simple biological functions?
->use PyBrain here?
-agent competition
-agent imitation
-NN noise in reinforcement learning (combine with something)
-imagination?

-deep learning for text (autoencoders? recurrent LSTM? something something)
-also word2vec as baseline can be cute
-topic models learn topics automatically from data -> see also omri abend work for language aquistion (go from text to semantics by treating syntax as latent)

-deep learning without parameters? something better than genetic alg?

-extending my work:
-using graphical models after the first step as recursive classifier (converting non-graphical problems to graphical ones! now you have a label propagation problem. can mby solve like pagerank algorithm?)
->can mby use tfidf as weights for labeling
-making the alg at least a bit more scalable (horrible complexity is horrible)
-idea (credit to omer levy): is there a way to generalize the relations used? (the classifier uses very specific relations, but it's possible that a very similar relation exist that can be used as well. could mby do this using schema tricks?)

Minor stuff:
-ideas models better than deterministic ones since can offer confidence->can rank positive results by confidence(like in watson), especially in relational case? see also topic models for recommendation/data mining
- crypto lens for learning (zero knowledge, private learning, learning over functional encryption)
-Deep learning with other weight schemes(not convolution/full connect)(dropout zeros some fraction of the weights and helps generalize so mby)? also other transforms?
-smart parameter tuning on anytime algorithms(mby use statistical tests to help with time)
