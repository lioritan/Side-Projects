{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow >= 1.9 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: Once you enable eager execution, it cannot be disabled. \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/yashkatariya/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/yashkatariya/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "text = unidecode.unidecode(open(path_to_file).read())\n",
    "# length of text is the number of characters in it\n",
    "print (len(text))\n",
    "# unique contains all the unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 100\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11153, 100)\n",
      "(11153, 100)\n",
      "WARNING:tensorflow:From <ipython-input-4-efa229c483f8>:14: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f+1:f+1+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(batch_size=BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.batch_sz = batch_size\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True, \n",
    "                                     recurrent_activation='sigmoid', \n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # output shape == (batch_size, max_length, hidden_size) \n",
    "    # states shape == (batch_size, hidden_size)\n",
    "\n",
    "    # states variable to preserve the state of the model\n",
    "    # this will be used to pass at every step to the model while training\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "\n",
    "\n",
    "    # reshaping the output so that we can pass it to the Dense layer\n",
    "    # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # The dense layer will output predictions for every time_steps(max_length)\n",
    "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1695\n",
      "Epoch 1 Batch 100 Loss 2.1673\n",
      "Epoch 1 Loss 1.9951\n",
      "Time taken for 1 epoch 1174.355381011963 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.9240\n",
      "Epoch 2 Batch 100 Loss 1.7606\n",
      "Epoch 2 Loss 1.6903\n",
      "Time taken for 1 epoch 1032.972647190094 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6533\n",
      "Epoch 3 Batch 100 Loss 1.5444\n",
      "Epoch 3 Loss 1.5352\n",
      "Time taken for 1 epoch 1030.8399131298065 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.4558\n",
      "Epoch 4 Batch 100 Loss 1.4525\n",
      "Epoch 4 Loss 1.4637\n",
      "Time taken for 1 epoch 1019.3054502010345 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.3907\n",
      "Epoch 5 Batch 100 Loss 1.3789\n",
      "Epoch 5 Loss 1.3920\n",
      "Time taken for 1 epoch 1037.3343195915222 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3339\n",
      "Epoch 6 Batch 100 Loss 1.3505\n",
      "Epoch 6 Loss 1.3729\n",
      "Time taken for 1 epoch 1035.5254275798798 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2685\n",
      "Epoch 7 Batch 100 Loss 1.3192\n",
      "Epoch 7 Loss 1.3207\n",
      "Time taken for 1 epoch 1038.3602838516235 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2238\n",
      "Epoch 8 Batch 100 Loss 1.2492\n",
      "Epoch 8 Loss 1.2666\n",
      "Time taken for 1 epoch 1065.1876459121704 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1235\n",
      "Epoch 9 Batch 100 Loss 1.2190\n",
      "Epoch 9 Loss 1.2607\n",
      "Time taken for 1 epoch 1441.918731212616 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1330\n",
      "Epoch 10 Batch 100 Loss 1.1619\n",
      "Epoch 10 Loss 1.2168\n",
      "Time taken for 1 epoch 1624.5265452861786 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.0685\n",
      "Epoch 11 Batch 100 Loss 1.1521\n",
      "Epoch 11 Loss 1.1479\n",
      "Time taken for 1 epoch 1628.5719792842865 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0658\n",
      "Epoch 12 Batch 100 Loss 1.0685\n",
      "Epoch 12 Loss 1.1177\n",
      "Time taken for 1 epoch 1644.5875489711761 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.9818\n",
      "Epoch 13 Batch 100 Loss 1.0577\n",
      "Epoch 13 Loss 1.0414\n",
      "Time taken for 1 epoch 1648.5339035987854 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.9207\n",
      "Epoch 14 Batch 100 Loss 1.0688\n",
      "Epoch 14 Loss 1.0654\n",
      "Time taken for 1 epoch 1660.587124824524 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.9090\n",
      "Epoch 15 Batch 100 Loss 1.0236\n",
      "Epoch 15 Loss 1.0472\n",
      "Time taken for 1 epoch 1726.7953419685364 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.8591\n",
      "Epoch 16 Batch 100 Loss 0.9623\n",
      "Epoch 16 Loss 0.9828\n",
      "Time taken for 1 epoch 1669.5266633033752 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.8414\n",
      "Epoch 17 Batch 100 Loss 0.9366\n",
      "Epoch 17 Loss 0.9782\n",
      "Time taken for 1 epoch 1713.5808050632477 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.7830\n",
      "Epoch 18 Batch 100 Loss 0.8776\n",
      "Epoch 18 Loss 0.9236\n",
      "Time taken for 1 epoch 1678.7940528392792 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.7340\n",
      "Epoch 19 Batch 100 Loss 0.8681\n",
      "Epoch 19 Loss 0.9012\n",
      "Time taken for 1 epoch 1574.3701367378235 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.7416\n",
      "Epoch 20 Batch 100 Loss 0.8410\n",
      "Epoch 20 Loss 0.8708\n",
      "Time taken for 1 epoch 1015.0595569610596 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.6894\n",
      "Epoch 21 Batch 100 Loss 0.8359\n",
      "Epoch 21 Loss 0.8697\n",
      "Time taken for 1 epoch 1014.5920464992523 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.6894\n",
      "Epoch 22 Batch 100 Loss 0.8011\n",
      "Epoch 22 Loss 0.8434\n",
      "Time taken for 1 epoch 1014.4523389339447 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.6948\n",
      "Epoch 23 Batch 100 Loss 0.7862\n",
      "Epoch 23 Loss 0.8617\n",
      "Time taken for 1 epoch 1014.7266185283661 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.6528\n",
      "Epoch 24 Batch 100 Loss 0.7845\n",
      "Epoch 24 Loss 0.8415\n",
      "Time taken for 1 epoch 1017.875617980957 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.6274\n",
      "Epoch 25 Batch 100 Loss 0.7903\n",
      "Epoch 25 Loss 0.7860\n",
      "Time taken for 1 epoch 1014.7340230941772 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.6317\n",
      "Epoch 26 Batch 100 Loss 0.7692\n",
      "Epoch 26 Loss 0.8090\n",
      "Time taken for 1 epoch 1013.918208360672 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.5804\n",
      "Epoch 27 Batch 100 Loss 0.7597\n",
      "Epoch 27 Loss 0.7767\n",
      "Time taken for 1 epoch 1014.0555167198181 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.6114\n",
      "Epoch 28 Batch 100 Loss 0.7109\n",
      "Epoch 28 Loss 0.7425\n",
      "Time taken for 1 epoch 1012.9030754566193 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.5900\n",
      "Epoch 29 Batch 100 Loss 0.7467\n",
      "Epoch 29 Loss 0.7646\n",
      "Time taken for 1 epoch 1014.1063311100006 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.5894\n",
      "Epoch 30 Batch 100 Loss 0.6999\n",
      "Epoch 30 Loss 0.7616\n",
      "Time taken for 1 epoch 1014.307460308075 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions, hidden = model(inp, hidden)\n",
    "              \n",
    "              # reshaping the target because that's how the \n",
    "              # loss function expects it\n",
    "              target = tf.reshape(target, (-1,))\n",
    "              loss = loss_function(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus to make looks,\n",
      "When they do hate the daughter of a fair uncle's son,\n",
      "Being put to pray for you to the world nor heaven,\n",
      "That seeming would be talk'd with peace,\n",
      "And over-ship more wondering in his breast.\n",
      "What shall we do?\n",
      "\n",
      "CLARENCE:\n",
      "\n",
      "WARWICK:\n",
      "I have some foot my gracious lord, I tell you, fellow, you may say.\n",
      "\n",
      "HORTENSIO:\n",
      "The master, my lord,\n",
      "I have seen them in the house of a man\n",
      "Your part in him: wherefore do I stay here be claim by on my knee,\n",
      "And that the entertainment was the duke made bold words in prodigy,\n",
      "He should take life to be the fault that thou hast been,\n",
      "And threatened weeds at home to endure\n",
      "My dearth to use my part from the heart to go.\n",
      "\n",
      "BAPTISTA:\n",
      "I must confess your grace to come, my gracious lord.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Ay, now be sent to them.\n",
      "\n",
      "BIONDELLO:\n",
      "Sir, my mistress.\n",
      "\n",
      "KING RICHARD III:\n",
      "Here pity that thou hast said enough commonwealth.\n",
      "What say you, Signior Gremio?\n",
      "\n",
      "GREMIO:\n",
      "And may not young men die.\n",
      "\n",
      "First Servingman:\n",
      "By my house we would have said for this mil\n"
     ]
    }
   ],
   "source": [
    "# Evaluation step(generating text using the model learned)\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'T'\n",
    "# converting our start string to numbers(vectorizing!) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# empty string to store our results\n",
    "text_generated = ''\n",
    "\n",
    "# low temperatures results in more predictable text.\n",
    "# higher temperatures results in more surprising text\n",
    "# experiment to find the best setting\n",
    "temperature = 2.0\n",
    "\n",
    "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated += idx2char[predicted_id]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
