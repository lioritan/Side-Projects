{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow >= 1.9 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: Once you enable eager execution, it cannot be disabled. \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file =  r'D:\\GitHub\\Side-Projects\\hmm_demo\\data\\test_data1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42420\n"
     ]
    }
   ],
   "source": [
    "text = unidecode.unidecode(open(path_to_file).read())\n",
    "# length of text is the number of characters in it\n",
    "print (len(text))\n",
    "# unique contains all the unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 120\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 128\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 512\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 120)\n",
      "(353, 120)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f+1:f+1+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(batch_size=BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.batch_sz = batch_size\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True, \n",
    "                                     recurrent_activation='sigmoid', \n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # output shape == (batch_size, max_length, hidden_size) \n",
    "    # states shape == (batch_size, hidden_size)\n",
    "\n",
    "    # states variable to preserve the state of the model\n",
    "    # this will be used to pass at every step to the model while training\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "\n",
    "\n",
    "    # reshaping the output so that we can pass it to the Dense layer\n",
    "    # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # The dense layer will output predictions for every time_steps(max_length)\n",
    "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.5419\n",
      "Epoch 1 Loss 2.5568\n",
      "Time taken for 1 epoch 28.42983388900757 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.5338\n",
      "Epoch 2 Loss 2.4479\n",
      "Time taken for 1 epoch 27.984103441238403 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.4711\n",
      "Epoch 3 Loss 2.4515\n",
      "Time taken for 1 epoch 28.62662982940674 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.4163\n",
      "Epoch 4 Loss 2.4052\n",
      "Time taken for 1 epoch 29.337477207183838 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.3668\n",
      "Epoch 5 Loss 2.3750\n",
      "Time taken for 1 epoch 28.971940517425537 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.3556\n",
      "Epoch 6 Loss 2.3287\n",
      "Time taken for 1 epoch 29.04780650138855 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.2694\n",
      "Epoch 7 Loss 2.3117\n",
      "Time taken for 1 epoch 29.122838020324707 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.2595\n",
      "Epoch 8 Loss 2.2596\n",
      "Time taken for 1 epoch 29.230953454971313 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.2687\n",
      "Epoch 9 Loss 2.1942\n",
      "Time taken for 1 epoch 28.901506185531616 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.2164\n",
      "Epoch 10 Loss 2.2007\n",
      "Time taken for 1 epoch 28.453840494155884 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.1686\n",
      "Epoch 11 Loss 2.1753\n",
      "Time taken for 1 epoch 29.440170764923096 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.0990\n",
      "Epoch 12 Loss 2.1277\n",
      "Time taken for 1 epoch 28.800952672958374 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 2.0692\n",
      "Epoch 13 Loss 2.0759\n",
      "Time taken for 1 epoch 28.371025323867798 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 2.0714\n",
      "Epoch 14 Loss 2.0495\n",
      "Time taken for 1 epoch 29.415270805358887 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 2.0193\n",
      "Epoch 15 Loss 2.0298\n",
      "Time taken for 1 epoch 29.214804649353027 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.9666\n",
      "Epoch 16 Loss 1.9937\n",
      "Time taken for 1 epoch 29.093130350112915 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.9519\n",
      "Epoch 17 Loss 1.9658\n",
      "Time taken for 1 epoch 28.43707036972046 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.9048\n",
      "Epoch 18 Loss 1.9277\n",
      "Time taken for 1 epoch 29.72625231742859 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.9021\n",
      "Epoch 19 Loss 1.8893\n",
      "Time taken for 1 epoch 28.533660411834717 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.8581\n",
      "Epoch 20 Loss 1.8535\n",
      "Time taken for 1 epoch 29.51002287864685 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.8595\n",
      "Epoch 21 Loss 1.8073\n",
      "Time taken for 1 epoch 28.49074363708496 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.8476\n",
      "Epoch 22 Loss 1.7899\n",
      "Time taken for 1 epoch 29.82550573348999 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.7815\n",
      "Epoch 23 Loss 1.8252\n",
      "Time taken for 1 epoch 29.13753628730774 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.7837\n",
      "Epoch 24 Loss 1.7554\n",
      "Time taken for 1 epoch 30.100502729415894 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.7249\n",
      "Epoch 25 Loss 1.7344\n",
      "Time taken for 1 epoch 30.595205068588257 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.6713\n",
      "Epoch 26 Loss 1.7076\n",
      "Time taken for 1 epoch 30.557610273361206 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.7000\n",
      "Epoch 27 Loss 1.6734\n",
      "Time taken for 1 epoch 30.194127798080444 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.6688\n",
      "Epoch 28 Loss 1.6353\n",
      "Time taken for 1 epoch 30.46005606651306 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.6302\n",
      "Epoch 29 Loss 1.6048\n",
      "Time taken for 1 epoch 30.652449131011963 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 1.6106\n",
      "Epoch 30 Loss 1.5948\n",
      "Time taken for 1 epoch 30.591050624847412 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions, hidden = model(inp, hidden)\n",
    "              \n",
    "              # reshaping the target because that's how the \n",
    "              # loss function expects it\n",
    "              target = tf.reshape(target, (-1,))\n",
    "              loss = loss_function(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aed a bug where the versions of the sement to 4: que the Elyound in the sement for the can now can seme to the will a bug where the Gement for the Parant Crantor Stite \n",
      "You could the skill gem to from the Kith from 20% increased the mant (down from 10%). The sement for the real the mant for the damage and the Oriand -ppere the Abyss now deal now crapter to now cranted to the Stare Acting a lighte skills which can now can now be the deal now seal damage the man increased the skill gem to the mant for the can now be the damage of the level requirement for the can now can now and the Elyor all Fill no longer can now ald the revel requirement for the atem a dumage of the sement for the 1tom for the /and a now deal sumport to the kill now grants while have been redured the pard dife and a dimate of the atem could to the sement to the best in the real damage the damage of the EtemVity (zor the skill gem to the pard now can now area to 70.\n",
      "Liget and a now the partion in the Level String and th\n"
     ]
    }
   ],
   "source": [
    "# Evaluation step(generating text using the model learned)\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'A'\n",
    "# converting our start string to numbers(vectorizing!) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# empty string to store our results\n",
    "text_generated = ''\n",
    "\n",
    "# low temperatures results in more predictable text.\n",
    "# higher temperatures results in more surprising text\n",
    "# experiment to find the best setting\n",
    "temperature = 2.0\n",
    "\n",
    "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated += idx2char[predicted_id]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
